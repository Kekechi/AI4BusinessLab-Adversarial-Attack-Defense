[json](data/json/007.json)

## 1. Main Question

- What problem is the paper trying to solve? (1–2 sentences)
Validity of Adversarial attack in large scale models
## 2. Key Method

- One-line description of the main attack/defense.
- Use glossary terms when possible (e.g., “FGSM = adds noise along gradient sign”).
label leak, Adversarial training makes the model more accurate on adversarial example than the clean image
## 3. Main Results

- One key experiment (dataset + outcome).
- Quote numbers if easy (e.g., “Error reduced from 0.94% → 0.84% on MNIST”).
Label leaking
Transferability is more likely on one step method like FGSM
## 4. Strengths & Limitations

- 1 bullet strength (e.g., “Simple and reproducible”).
- 1 bullet limitation (e.g., “Only tested on images, white-box only”).

## 5. Applications / Relevance

- If any — real-world link, or why it matters for other research.

## 6. Self-Check

- **2-minute explanation**: Could you explain this to a classmate? Write 2–3 plain sentences.
- **Unknown terms**: List any jargon you didn’t fully get (e.g., “maxout network,” “universal approximator theorem”).
The paper show interest results about label leaking and transferability of adversarial example with experiments on large scale models like Inception v3 trained on ImageNet.